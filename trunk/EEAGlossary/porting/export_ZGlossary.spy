## Script (Python) "export ZGlossary product"
##bind container=container
##bind context=context
##bind namespace=
##bind script=script
##bind subpath=traverse_subpath
##parameters=glossary_center_id
##title=ZGlossary export

# imports
import base64

#get the HTML request and response objects.
request = container.REQUEST
RESPONSE =  request.RESPONSE

#set content type
RESPONSE.setHeader('content-type', 'text/xml')

#####################
# load default data #
#####################
messages = []
messages.append("""<?xml version="1.0" encoding="utf-8"?>\n<export>""")

glossary_objects = ["GlossaryFolder", "GlossaryElement", "GlossaryElementSynonym"]
additional_objects =["GlossaryNews", "GlossaryCenter"]
glossary_objects.extend(additional_objects)

root = container.restrictedTraverse('/')

####################
# export functions #
####################
def exportGlossaryContent(glossary_center):
    for item in glossary_center.objectValues(glossary_objects):
        if item.meta_type == "GlossaryFolder":
            messages.append('%s' % (exportGlossaryFolder(item)))

            for glossary_item in item.objectValues(["GlossaryElement", "GlossaryElementSynonym"]):
                if glossary_item.meta_type == "GlossaryElement":
                    messages.append('%s' % (exportGlossaryElement(glossary_item)))
                elif glossary_item.meta_type == "GlossaryElementSynonym":
                     messages.append('%s' % (exportGlossarySynonym(glossary_item)))

            messages.append('\n</GFolder>')

def exportGlossaryFolder(p_item):
    #folders start
    l_temp = ''
    l_temp += '\n<GFolder'

    #folders properties
    l_temp += ' id="%s"' % utXmlEncode(p_item.id)
    l_temp += ' title="%s"' % utXmlEncode(p_item.title)
    l_temp += ' description="%s"' % utXmlEncode(p_item.description)
    l_temp += '>'
    return l_temp

def exportGlossaryElement(p_item):
    #elements start
    l_temp = ''
    l_temp += '\n<GElement'

#    #elements properties
    l_temp += ' id="%s"' % utXmlEncode(p_item.id)
#    l_temp += ' name="%s"' % utXmlEncode(p_item.name)
#    l_temp += ' type="%s"' % utXmlEncode(p_item.type) #selection
#    l_temp += ' source="%s"' % utXmlEncode(p_item.source)
#    l_temp += ' context="%s"' % utXmlEncode(p_item.context)
#    l_temp += ' comment="%s"' % utXmlEncode(p_item.comment)
#    l_temp += ' used_for_1="%s"' % utXmlEncode(p_item.used_for_1)
#    l_temp += ' used_for_2="%s"' % utXmlEncode(p_item.used_for_2)
#
#    #XXX to be fixed
#    if p_item.id == 'broadleaved_woodland':
#        #l_def = p_item.definition
#        #l_def = unicode(l_def, 'latin-1')
#        #l_def = l_def.replace(u'\u00a0', ' ')
#        #l_def.encode('latin-1')
#        l_temp += ' definition="%s"' % 'Wooded land on which more than 75 % of the tree crown cover consists of broadleaved species.'
#    elif p_item.id == 'coniferous_woodland':
#        l_temp += ' definition="%s"' % 'Coniferous woodland is defined as wooded land on which more than 75 % of the tree crown cover consists of coniferous species.'
#    elif p_item.id == 'forest':
#        l_temp += ' definition="%s"' % 'Land with a tree canopy cover of more than 10 % and an area of more than 0.5 ha.'
#    elif p_item.id == 'grassland_and_tall_forb_habitats':
#        l_temp += ' definition="%s"' % 'One of the main habitat types in the EUNIS habitats classification. Non-coastal habitats which are dry or only seasonally wet (with the water table at or above ground level for less than half of the year) with greater than 30 % vegetation cover. The dominant vegetation is grasses and other non-woody vegetation (including moss-, lichen-, fern- and sedge-dominated communities). Includes successional weedy communities and managed grasslands such as recreation fields and lawns. Does not include regularly tilled habitats dominated by cultivated herbaceous vegetation such as arable fields.'
#    elif p_item.id == 'heathland,_scrub_and_tundra_habitats':
#        l_temp += ' definition="%s"' % 'One of the main habitat types in the EUNIS habitats classification. Non-coastal habitats which are dry or only seasonally wet (with the water table at or above ground level for less than half of the year) with greater than 30 % vegetation cover. The dominant vegetation is shrubs or dwarf shrubs. Includes regularly tilled shrub orchards, hedges (which may have occasional tall trees) and habitats characterised by the presence of permafrost. Also includes dwarf trees and scrub (under 50 cm, such as occur in extreme alpine conditions).'
#    elif p_item.id == 'inland_unvegetated_or_sparsely_vegetated_habitats':
#        l_temp += ' definition="%s"' % 'One of the main habitat types in the EUNIS habitats classification. They are non-coastal habitats with less than 30 % vegetation cover (other than where the vegetation is chasmophytic or on scree and or cliff) which are dry or only seasonally wet (with the water table at or above ground level for less than half of the year). Subterranean non-marine caves and passages including underground waters. Habitats characterised by the presence of permanent snow and surface ice other than marine ice bodies.'
#    elif p_item.id == 'mixed_woodland':
#        l_temp += ' definition="%s"' % 'Wooded land on which neither coniferous, nor broadleaved species account for more than 75 % of the crown cover.'
#    elif p_item.id == 'mixed_woodland':
#        l_temp += ' definition="%s"' % 'Wooded land on which neither coniferous, nor broadleaved species account for more than 75 % of the crown cover.'
#    else:
#        l_temp += ' definition="%s"' % utXmlEncode(p_item.definition)
#
#    #XXX to be fixed
#    if p_item.id == 'tropical_forest':
#        l_temp += ' definition_source="%s"' % 'Council Regulation (EC) No 3062/95 of 20 December 1995 on operations to promote tropical forests (Article 2)'
#    elif p_item.id == 'utilised_agriculture_area':
#        l_temp += ' definition_source="%s"' % (unicode("Eurostat/Ministére de l'Agriculture e de la Pêche. Agriculture in the European Union. Èditions Agreste. France.", 'latin-1')).encode('utf-8')
#    else:
#        l_temp += ' definition_source="%s"' % utXmlEncode(p_item.definition_source)
#
#    l_temp += ' subjects="%s"' % utXmlEncode(p_item.subjects) #multiple select
#    l_temp += ' disabled="%s"' % utXmlEncode(p_item.disabled) #boolean
#    l_temp += ' approved="%s"' % utXmlEncode(p_item.approved) #boolean
#    l_temp += ' long_definition="%s"' % utXmlEncode(p_item.long_definition)
#    l_temp += ' QA_needed="%s"' % utXmlEncode(p_item.QA_needed) #boolean
#    l_temp += ' definition_source_url="%s"' % utXmlEncode(p_item.definition_source_url)
#    l_temp += ' definition_source_year="%s"' % utXmlEncode(p_item.definition_source_year)
#    l_temp += ' definition_source_org="%s"' % utXmlEncode(p_item.definition_source_org)
#    l_temp += ' definition_source_org_full_name="%s"' % utXmlEncode(p_item.definition_source_org_full_name)


    #elements translations
#    l_temp += ' Bulgarian="%s"' % utHtmlEncode(p_item.Bulgarian, 0) # - D
##    l_temp += ' Bulgarian="%s"' % to_utf8(p_item.Bulgarian, 'iso-8859-5')
##    l_temp += ' Bulgarian="%s"' % (unicode(p_item.Bulgarian, 'iso-8859-5')).encode('utf-8')
#
#    l_temp += ' Croatian="%s"' % utHtmlEncode(p_item.Croatian) # - E D
##    l_temp += ' Croatian="%s"' % to_utf8(p_item.Croatian, 'iso-8859-2')
#
#    l_temp += ' Czech="%s"' % utXmlEncode(p_item.Czech, 'utf-8', 0) # - D
##    l_temp += ' Czech="%s"' % to_utf8(p_item.Czech, 'iso-8859-2')
##    l_temp += ' Czech="%s"' % (unicode(p_item.Czech, 'iso-8859-2')).encode('utf-8')
#
##    l_temp += ' Danish="%s"' % p_item.Danish # - X
##    l_temp += ' Danish="%s"' % to_utf8(p_item.Danish, 'iso-8859-1')
##    l_temp += ' Danish="%s"' % (unicode(p_item.Danish, 'iso-8859-1')).encode('utf-8')
#
##    l_temp += ' Dutch="%s"' % utXmlEncode(p_item.Dutch, 'utf-8', 0) # - X
##    l_temp += ' Dutch="%s"' % to_utf8(p_item.Dutch, 'iso-8859-1')
##    l_temp += ' Dutch="%s"' % (unicode(p_item.Dutch, 'iso-8859-1')).encode('utf-8')
#
#    l_temp += ' English="%s"' % utXmlEncode(p_item.English) # - D
#
#    l_temp += ' Estonian="%s"' % utXmlEncode(p_item.Estonian, 'utf-8', 0) # - D
##    l_temp += ' Estonian="%s"' % to_utf8(p_item.Estonian, 'iso-8859-4')
##    l_temp += ' Estonian="%s"' % (unicode(p_item.Estonian, 'iso-8859-4')).encode('utf-8')
#
##    l_temp += ' Finnish="%s"' % utXmlEncode(p_item.Finnish, 'utf-8', 0) # - X
##    l_temp += ' Finnish="%s"' % p_item.Finnish
##    l_temp += ' Finnish="%s"' % to_utf8(p_item.Finnish, 'iso-8859-1')
##    l_temp += ' Finnish="%s"' % (unicode(p_item.Finnish, 'iso-8859-1')).encode('utf-8')
#
##    l_temp += ' French="%s"' % to_utf8(p_item.French, 'iso-8859-1') # - X
##    l_temp += ' French="%s"' % (unicode(p_item.French, 'iso-8859-1')).encode('utf-8')
##    l_temp += ' French="%s"' % p_item.French
#
##    l_temp += ' German="%s"' % to_utf8(p_item.German, 'iso-8859-1') # - X
##    l_temp += ' German="%s"' % (unicode(p_item.German, 'iso-8859-1')).encode('utf-8')
##    l_temp += ' German="%s"' % p_item.German
#
#    if p_item.id in ['bottom-up_approach', 'polluter_pays_principle', 'top-down_approach', 'user-pays_principle']:  # - D
#        l_temp += ' Greek="%s"' % 'TO BE COMPLETED ...'
#    else:
#        l_temp += ' Greek="%s"' % utXmlEncode(p_item.Greek, 'iso-8859-7', 0)
##    l_temp += ' Greek="%s"' % to_utf8(p_item.Greek, 'iso-8859-7')
##    l_temp += ' Greek="%s"' % p_item.Greek
#
    l_temp += ' Hungarian="%s"' % utXmlEncode(p_item.Hungarian, 'utf-8', 0) # - D
##    l_temp += ' Hungarian="%s"' % to_utf8(p_item.Hungarian, 'iso-8859-2')
##    l_temp += ' Hungarian="%s"' % (unicode(p_item.Hungarian, 'iso-8859-2')).encode('utf-8')
#
##    l_temp += ' Icelandic="%s"' % to_utf8(p_item.Icelandic, 'iso-8859-1')
#
##    l_temp += ' Italian="%s"' % to_utf8(p_item.Italian, 'iso-8859-1')
#
##    l_temp += ' Latvian="%s"' % to_utf8(p_item.Latvian, 'iso-8859-4')
#
##    l_temp += ' Lithuanian="%s"' % to_utf8(p_item.Lithuanian, 'iso-8859-4')
#
#    l_temp += ' Macedonian="%s"' % utHtmlEncode(p_item.Macedonian) # - E D
##    l_temp += ' Macedonian="%s"' % to_utf8(p_item.Macedonian, 'iso-8859-5')
#
#    l_temp += ' Maltese="%s"' % utXmlEncode(p_item.Maltese, 'iso-8859-3', 0) # - D
##    l_temp += ' Maltese="%s"' % to_utf8(p_item.Maltese, 'iso-8859-3')
#
##    l_temp += ' Norwegian="%s"' % to_utf8(p_item.Norwegian, 'iso-8859-1')
#
##    l_temp += ' Polish="%s"' % to_utf8(p_item.Polish, 'iso-8859-2')
#
##    l_temp += ' Portuguese="%s"' % to_utf8(p_item.Portuguese, 'iso-8859-1')
#
##    l_temp += ' Romanian="%s"' % to_utf8(p_item.Romanian, 'iso-8859-2')
#
#    l_temp += ' Russian="%s"' % utHtmlEncode(p_item.Russian) # - E D
##    l_temp += ' Russian="%s"' % to_utf8(p_item.Russian, 'iso-8859-5')
#
#    l_temp += ' Serbian="%s"' % utHtmlEncode(p_item.Serbian) # - E D
##    l_temp += ' Serbian="%s"' % to_utf8(p_item.Serbian, 'iso-8859-5')
#
##    l_temp += ' Slovak="%s"' % to_utf8(p_item.Slovak, 'iso-8859-2')
#
##    l_temp += ' Slovenian="%s"' % to_utf8(p_item.Slovenian, 'iso-8859-2')
#
##    l_temp += ' Spanish="%s"' % to_utf8(p_item.Spanish, 'iso-8859-1')
#
##    l_temp += ' Swedish="%s"' % to_utf8(p_item.Swedish, 'iso-8859-1')
#
##    l_temp += ' Turkish="%s"' % to_utf8(p_item.Turkish, 'iso-8859-9')


    #elements end
    l_temp += ' />'
    return l_temp

def exportGlossarySynonym(p_item):
    #synonyms start
    l_temp = ''
    l_temp = '\n<GSynonym>'

    #synonyms properties
    #XXX...to be completed

    #synonyms end
    l_temp+=' \n</GSynonym>'
    return l_temp

def utXmlEncode(p_string, p_charset = 'latin-1', p_special = 1):
    """Encode some special chars"""
    l_tmp = str(p_string)

    l_tmp = utHtmlEncode(l_tmp, p_special)
    l_tmp = utSpecialCharsEncode(l_tmp)
    if p_charset != 'none':
        l_tmp = (unicode(l_tmp, p_charset)).encode('utf-8')

    return l_tmp

def utHtmlEncode(p_string, p_special = 1):
    """ """
    l_tmp = str(p_string)

    if p_special:
        l_tmp = l_tmp.replace('&', '&amp;')
    l_tmp = l_tmp.replace('<', '&lt;')
    l_tmp = l_tmp.replace('"', '&quot;')
    l_tmp = l_tmp.replace('\'', '&apos;')
    l_tmp = l_tmp.replace('>', '&gt;')

    return l_tmp

def utSpecialCharsEncode(p_string):
    """ """
    l_tmp = str(p_string)

    l_tmp = l_tmp.replace('', '-')
    l_tmp = l_tmp.replace('', '-')
    l_tmp = l_tmp.replace('', "'")
    l_tmp = l_tmp.replace('', "'")
    l_tmp = l_tmp.replace(' ', " ") # - u'\u00a0' (english)
    l_tmp = l_tmp.replace('´', "'")
    l_tmp = l_tmp.replace("Â'", "'")
    l_tmp = l_tmp.replace('', '&quot;')
    l_tmp = l_tmp.replace('', '&quot;')
    l_tmp = l_tmp.replace('§', "-")
    l_tmp = l_tmp.replace('«', "'")
    l_tmp = l_tmp.replace('»', "'") # - u'\u00bb' (greek)

    return l_tmp


##########
# toutf8 #
##########
import string

##
## ISO-8859-4
##

iso8859_4 = [
# /* 0xa0 */
  0x00a0, 0x0104, 0x0138, 0x0156, 0x00a4, 0x0128, 0x013b, 0x00a7,
  0x00a8, 0x0160, 0x0112, 0x0122, 0x0166, 0x00ad, 0x017d, 0x00af,
# /* 0xb0 */
  0x00b0, 0x0105, 0x02db, 0x0157, 0x00b4, 0x0129, 0x013c, 0x02c7,
  0x00b8, 0x0161, 0x0113, 0x0123, 0x0167, 0x014a, 0x017e, 0x014b,
# /* 0xc0 */
  0x0100, 0x00c1, 0x00c2, 0x00c3, 0x00c4, 0x00c5, 0x00c6, 0x012e,
  0x010c, 0x00c9, 0x0118, 0x00cb, 0x0116, 0x00cd, 0x00ce, 0x012a,
# /* 0xd0 */
  0x0110, 0x0145, 0x014c, 0x0136, 0x00d4, 0x00d5, 0x00d6, 0x00d7,
  0x00d8, 0x0172, 0x00da, 0x00db, 0x00dc, 0x0168, 0x016a, 0x00df,
# /* 0xe0 */
  0x0101, 0x00e1, 0x00e2, 0x00e3, 0x00e4, 0x00e5, 0x00e6, 0x012f,
  0x010d, 0x00e9, 0x0119, 0x00eb, 0x0117, 0x00ed, 0x00ee, 0x012b,
# /* 0xf0 */
  0x0111, 0x0146, 0x014d, 0x0137, 0x00f4, 0x00f5, 0x00f6, 0x00f7,
  0x00f8, 0x0173, 0x00fa, 0x00fb, 0x00fc, 0x0169, 0x016b, 0x02d9,
]

##
## ISO-8859-2
##
 
iso8859_2 = [
# /* 0xa0 */
  0x00a0, 0x0104, 0x02d8, 0x0141, 0x00a4, 0x013d, 0x015a, 0x00a7,
  0x00a8, 0x0160, 0x015e, 0x0164, 0x0179, 0x00ad, 0x017d, 0x017b,
# /* 0xb0 */
  0x00b0, 0x0105, 0x02db, 0x0142, 0x00b4, 0x013e, 0x015b, 0x02c7,
  0x00b8, 0x0161, 0x015f, 0x0165, 0x017a, 0x02dd, 0x017e, 0x017c,
# /* 0xc0 */
  0x0154, 0x00c1, 0x00c2, 0x0102, 0x00c4, 0x0139, 0x0106, 0x00c7,
  0x010c, 0x00c9, 0x0118, 0x00cb, 0x011a, 0x00cd, 0x00ce, 0x010e,
# /* 0xd0 */
  0x0110, 0x0143, 0x0147, 0x00d3, 0x00d4, 0x0150, 0x00d6, 0x00d7,
  0x0158, 0x016e, 0x00da, 0x0170, 0x00dc, 0x00dd, 0x0162, 0x00df,
# /* 0xe0 */
  0x0155, 0x00e1, 0x00e2, 0x0103, 0x00e4, 0x013a, 0x0107, 0x00e7,
  0x010d, 0x00e9, 0x0119, 0x00eb, 0x011b, 0x00ed, 0x00ee, 0x010f,
# /* 0xf0 */
  0x0111, 0x0144, 0x0148, 0x00f3, 0x00f4, 0x0151, 0x00f6, 0x00f7,
  0x0159, 0x016f, 0x00fa, 0x0171, 0x00fc, 0x00fd, 0x0163, 0x02d9,
]

def to_utf8(s, charset):
    strlist = s
    res = ""
    if (charset == 'iso-8859-7'):
          for ch in strlist:
             if(ord(ch) >= 0xb4):
                    res = res + "&#" + `ord(ch) + 720` + ";"
             else:
                    res = res + ch

    elif (charset == 'iso-8859-5'):
            for ch in strlist:
                if(ord(ch) >= 0xa0):
                    res = res + "&#" + `ord(ch) + 864` + ";"
                else:
                    res = res + ch

    elif (charset == 'iso-8859-4'):
            for ch in strlist:
                if(ord(ch) >= 0xa0):
                    res = res + "&#" + `iso8859_4[ord(ch) - 0xa0]` + ";"
                else:
                    res = res + ch
    elif (charset == 'iso-8859-2'):
            for ch in strlist:
                if(ord(ch) >= 0xa0):
                    res = res + "&#" + `iso8859_2[ord(ch) - 0xa0]` + ";"
                else:
                    res = res + ch
    else:               #Assume iso-8859-1 if unknown charset
            for ch in strlist:
                if(ord(ch) >= 0x96):
                    res = res + "&#" + `ord(ch)` + ";"
                else:
                    res = res + ch
    return res



##################
# script content #
##################
for glossary_center in root.objectValues("GlossaryCenter"):
    if glossary_center.id==glossary_center_id:
        exportGlossaryContent(glossary_center)

messages.append("""\n</export>""")
print ''.join(messages)
return printed